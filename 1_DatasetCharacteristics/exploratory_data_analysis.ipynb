{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "## Table of Contents\n",
    "1. [Dataset Overview](#dataset-overview)\n",
    "2. [Handling Missing Values](#handling-missing-values)\n",
    "3. [Feature Distributions](#feature-distributions)\n",
    "4. [Possible Biases](#possible-biases)\n",
    "5. [Correlations](#correlations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". [Correlations](#correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "[Provide a high-level overview of the dataset. This should include the source of the dataset, the number of samples, the number of features, and example showing the structure of the dataset.]\n",
    "\n",
    "- Lifecycle of 4,500 Clouds\n",
    "- Simulated with the Weather Forecast Model ICON\n",
    "- Located above the Atlantic Ocean during the formation of Hurricane [Paulette](https://zoom.earth/storms/paulette-2020/)\n",
    "\n",
    "### Background\n",
    "A common method in cloud research is the application of cloud-tracking tools to study cloud life cycles and trajectories. The underlying data typically come from satellite imagery or from numerical weather prediction models. Clouds, however, are ephemeral, ever-changing objects, constantly shifting in shape and form. Tracking such transient features is therefore a challenging task.\n",
    "\n",
    "The temporal resolution of satellite imagery is limited to a best case of around 5 minutes. Numerical models are not restricted in the same way, but they are limited by the amount of data that can realistically be written and stored. For cloud studies, this temporal resolution becomes a severe bottleneck: short-lived clouds may appear and disappear between the standard model output intervals of 15 to 60 minutes.\n",
    "\n",
    "To overcome this limitation, the Leibniz Institute for Tropospheric Research ([TROPOS](https://www.tropos.de)) is developing - within the EU-funded CleanCloud project — the software Targo (Targeted Output). Targo enables cloud tracking in simulations at extremely high temporal resolution of around 30 seconds.\n",
    "\n",
    "### Method\n",
    "Targo is attached as a plugin to the weather forecast model ICON via the CoMIn interface. It requests meteorological fields at every model time step (typically 30–60 seconds) and performs cloud tracking using the community software tobac. At every cloud position, additional meteorological variables are requested, allowing for a comprehensive characterization of each cloud. This approach provides highly resolved cloud data at minimal storage cost.\n",
    "\n",
    "### Casestudy\n",
    "The study object is Hurricane Paulette from September 2020. A hurricane is one of the most extreme meteorological phenomena on our planet. It is characterized by a strong pressure depression (the hurricane eye), surrounded by a ring of convective clouds, intense precipitation, and thunderstorms.\n",
    "\n",
    "We applied our new cloud-tracking method to Paulette for a 48-hour period on 7–8 September 2020. From this simulation, we obtained 4,500 tracks of convective clouds. \n",
    "An example of identified cloud objects and an overfiew or the cloud trajectories can be found in the following figures.\n",
    "\n",
    "![Identified Cloud Objects](Cloud_Features.png) ![Cloud Tracks](Tracks.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structure\n",
    "A cloud may exist in multiple frames (time steps) of the simulation. Coresponding clouds in the frames are linked together to cells. In other words, a cell is a time series of a cloud.\n",
    "Our dataset consist of these cells.\n",
    "\n",
    "Each cell is stored as an indevidual dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "# Replace 'your_dataset.csv' with the path to your actual dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Number of samples\n",
    "num_samples = df.shape[0]\n",
    "\n",
    "# Number of features\n",
    "num_features = df.shape[1]\n",
    "\n",
    "# Display these dataset characteristics\n",
    "print(f\"Number of samples: {num_samples}\")\n",
    "print(f\"Number of features: {num_features}\")\n",
    "\n",
    "# Display the first few rows of the dataframe to show the structure\n",
    "print(\"Example data:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "[Identify any missing values in the dataset, and describe your approach to handle them if there are any. If there are no missing values simply indicate that there are none.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "# Example: Replacing NaN values with the mean value of the column\n",
    "# df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Your code for handling missing values goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Distributions\n",
    "\n",
    "[Plot the distribution of various features and target variables. Comment on the skewness, outliers, or any other observations.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plotting histograms of all numerical features\n",
    "df.hist(figsize=(12, 12))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Biases\n",
    "\n",
    "[Investigate the dataset for any biases that could affect the model’s performance and fairness (e.g., class imbalance, historical biases).]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Checking for class imbalance in a classification problem\n",
    "# sns.countplot(x='target_variable', data=df)\n",
    "\n",
    "# Your code to investigate possible biases goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n",
    "[Explore correlations between features and the target variable, as well as among features themselves.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plotting a heatmap to show feature correlations\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tobac-aod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
