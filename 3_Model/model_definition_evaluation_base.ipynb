{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4062bfe4",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Setup](#setup)\n",
    "2. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc4c9a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2392e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os, json, hashlib\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from huggingface_hub import login, list_repo_files, hf_hub_download, upload_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60dab26",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e0738",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ID = \"mttfst/Paulette_Cloud_Tracks\"\n",
    "token = \"\"\n",
    "\n",
    "USER = \"xx\"\n",
    "\n",
    "WINDOW_MINUTES =  30 #Minuten\n",
    "CUTOFF_STEPS = 5\n",
    "WINDOW_STEPS  = 60        # 30 min × 60 s / 30 s = 60 Steps\n",
    "N_FEATURES    = 20        # 8 Skalare + 12 TS-Features\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 40\n",
    "LR            = 1e-3\n",
    "MODEL_TYPE    = \"SimpleRNN\"   \n",
    "LOSS = \"MSE\"\n",
    "UNITS1 = 64\n",
    "UNITS2 = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e77171e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Login HuggingFace\n",
    "# =========================================\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8660a",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a502f1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_ID: simple_rnn_2026-02-18_08-30-39_b1c365f94d\n",
      "[HF] upload skipped/failed (setup): The read operation timed out\n"
     ]
    }
   ],
   "source": [
    "# Du kannst hier ein separates Repo für Logs/Configs nutzen (empfohlen),\n",
    "# oder du lässt es auf dem Dataset-Repo laufen.\n",
    "CONFIG_REPO_ID = REPO_ID  # z.B. \"thorsten789/hurricane_cloud_runs\"\n",
    "\n",
    "def make_run_id(prefix: str, config: dict) -> str:\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    cfg_str = json.dumps(config, sort_keys=True)\n",
    "    h = hashlib.sha1(cfg_str.encode(\"utf-8\")).hexdigest()[:10]\n",
    "    return f\"{prefix}_{ts}_{h}\"\n",
    "\n",
    "def save_json_local(path: str, data: dict) -> str:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    return path\n",
    "\n",
    "def upload_json_hf(local_path: str, run_id: str, name: str, base_dir: str = \"runs\", user= USER):\n",
    "    \"\"\"Lädt JSON als {base_dir}/{user}/{run_id}/{name}.json in CONFIG_REPO_ID hoch.\"\"\"\n",
    "    try:\n",
    "        path_in_repo = f\"{base_dir}/{user}/{run_id}/{name}.json\"\n",
    "        upload_file(\n",
    "            path_or_fileobj=local_path,\n",
    "            path_in_repo=path_in_repo,\n",
    "            repo_id=CONFIG_REPO_ID,\n",
    "            repo_type=\"dataset\",\n",
    "            commit_message=f\"Add {name}.json for {run_id}\",\n",
    "        )\n",
    "        print(f\"[HF] uploaded: {path_in_repo}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[HF] upload skipped/failed ({name}): {e}\")\n",
    "\n",
    "# --- zentrale Run-Config (die ID basiert auf config -> sinnvoller Run-Name)\n",
    "RUN_CONFIG = {\n",
    "    \"model\": MODEL_TYPE,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"units1\": UNITS1,\n",
    "    \"units2\": UNITS2,\n",
    "    \"lr\": LR,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"cutoff_steps\": CUTOFF_STEPS,\n",
    "    \"loss\": LOSS,\n",
    "}\n",
    "\n",
    "RUN_ID = make_run_id(\"simple_rnn\", RUN_CONFIG)\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "\n",
    "# Setup sofort speichern (damit du schon am Anfang einen Run hast)\n",
    "setup = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"config\": RUN_CONFIG,\n",
    "    \"data\": {\"repo_id\": REPO_ID},\n",
    "    \"meta\": {\"notebook\": \"3_Model/model_definition_evaluation_JS.ipynb\"},\n",
    "}\n",
    "\n",
    "# +++ Logging: Save Setup +++\n",
    "local_setup = save_json_local(f\"runs_local/{RUN_ID}/setup.json\", setup)\n",
    "upload_json_hf(local_setup, RUN_ID, \"setup\")\n",
    "\n",
    "class AutoSaveTrain(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Speichert train.json am Ende von model.fit (History + best_val_loss).\"\"\"\n",
    "    def __init__(self, run_id: str):\n",
    "        super().__init__()\n",
    "        self.run_id = run_id\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        hist = getattr(self.model, \"history\", None)\n",
    "        history_dict = hist.history if hist is not None else {}\n",
    "\n",
    "        train_data = {\n",
    "            \"run_id\": self.run_id,\n",
    "            \"history\": history_dict,\n",
    "            \"summary\": {\n",
    "                \"best_val_loss\": float(min(history_dict[\"val_loss\"])) if \"val_loss\" in history_dict else None,\n",
    "                \"final_train_loss\": float(history_dict[\"loss\"][-1]) if \"loss\" in history_dict and len(history_dict[\"loss\"]) else None,\n",
    "            },\n",
    "        }\n",
    "        local_train = save_json_local(f\"runs_local/{self.run_id}/train.json\", train_data)\n",
    "        upload_json_hf(local_train, self.run_id, \"train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0d30bf",
   "metadata": {},
   "source": [
    "### Load Data from HuggingFace\n",
    "\n",
    "TRACK LENGTH FILE:\n",
    "\n",
    "Diese CSV enthält Metadaten über alle Wolken:\n",
    "- filename: Dateiname des Tracks\n",
    "- track_len: Anzahl Zeitschritte (Lebensdauer)\n",
    "\n",
    "WARUM WICHTIG?\n",
    "- Wir trainieren nur auf Wolken mit >= 120 Zeitschritten\n",
    "- Grund: Kurze Wolken (<60 Minuten) sind zu variabel/chaotisch\n",
    "- Längere Wolken zeigen klare Lebenszyklen\n",
    "\n",
    "DATENQUELLE:\n",
    "Diese Datei wurde vorberechnet aus allen Tracks.\n",
    "Spart Zeit beim Training-Setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91282e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️  Downloading from HF: mttfst/Paulette_Cloud_Tracks/track_len/track_len_exp_1.1.csv\n",
      "✅ Loading track_len from HF-cached file: /root/.cache/huggingface/hub/datasets--mttfst--Paulette_Cloud_Tracks/snapshots/c1a773e1c4e6a3e79d72c16fcd3f4745828de02a/track_len/track_len_exp_1.1.csv\n",
      "tracks_120 shape: (9227, 2)\n",
      "Total CSV tracks with at least 120 timesteps: 1115\n",
      "Train: 150, Val: 25, Test: 25\n"
     ]
    }
   ],
   "source": [
    "tracks_120 = None\n",
    "\n",
    "# Wenn lokal nicht da: von Hugging Face Dataset-Repo herunterladen\n",
    "if tracks_120 is None:\n",
    "    from huggingface_hub import hf_hub_download\n",
    "\n",
    "    filename_in_repo = \"track_len/track_len_exp_1.1.csv\"\n",
    "\n",
    "    print(f\"⬇️  Downloading from HF: {REPO_ID}/{filename_in_repo}\")\n",
    "    local_file = hf_hub_download(\n",
    "        repo_id=REPO_ID,\n",
    "        repo_type=\"dataset\",\n",
    "        filename=filename_in_repo,\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Loading track_len from HF-cached file: {local_file}\")\n",
    "    tracks_120 = pd.read_csv(local_file)\n",
    "\n",
    "print(\"tracks_120 shape:\", tracks_120.shape)\n",
    "tracks_120.head()\n",
    "\n",
    "# =========================================\n",
    "# Dataset-Split\n",
    "# =========================================\n",
    "files = list_repo_files(REPO_ID, repo_type=\"dataset\")\n",
    "csv_files = [f for f in files if f.startswith(\"exp_1.1/\") and f.endswith(\".csv\")]\n",
    "\n",
    "tracks_120 = tracks_120[tracks_120.track_len >= 120]\n",
    "print(\"Total CSV tracks with at least 120 timesteps:\", len(tracks_120))\n",
    "\n",
    "tracks_120 = tracks_120.filename.to_list()\n",
    "\n",
    "csv_files = [\n",
    "    f for f in csv_files\n",
    "    if f.split(\"/\")[1] in tracks_120\n",
    "]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "n = len(csv_files)\n",
    "train_files = csv_files[: int(0.7 * n)]\n",
    "val_files   = csv_files[int(0.7 * n): int(0.85 * n)]\n",
    "test_files  = csv_files[int(0.85 * n):]\n",
    "\n",
    "print(f\"Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1d9c3",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "[Discuss the type(s) of models you consider for this task, and justify the selection.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed006b1b",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "[Describe any additional feature engineering you've performed beyond what was done for the baseline model.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b70f6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILE_PREFIXES = [\"qr_\", \"qc_\", \"qi_\", \"qs_\", \"qg_\", \"qv_\", \"roh_\", \"w_\"]\n",
    "\n",
    "SCALAR_FEATURES = [\n",
    "    \"cape_ml_L00\", \"cin_ml_L00\",\n",
    "    \"lwp_L00\",\n",
    "    \"iwp_L00\", \"rain_gsp_rate_L00\",\n",
    "    \"tqc_L00\", \"tqi_L00\", \"area_m2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a2e6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_remaining_lifetime(df, timestep_seconds=30):\n",
    "    n = len(df)\n",
    "    remaining_seconds = [(n - i - 1) * timestep_seconds for i in range(n)]\n",
    "    remaining_minutes = [s / 60 for s in remaining_seconds]\n",
    "    return remaining_minutes\n",
    "\n",
    "#def compute_future_rain(df, timestep_minutes=5):\n",
    "#    rain = df[\"rain_gsp_rate_L00\"].values\n",
    "#    dt = timestep_minutes * 60  # Sekunden\n",
    "\n",
    "#    future_rain = []\n",
    "#    for i in range(len(rain)):\n",
    "#        future_rain.append(rain[i:].sum() * dt)\n",
    "\n",
    "#    return future_rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bff74504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_profile(df, prefix, n_levels=50):\n",
    "    \"\"\"\n",
    "    Extrahiert ein vertikales Profil mit exakt n_levels.\n",
    "    Fehlende Level werden mit 0 aufgefüllt.\n",
    "    \"\"\"\n",
    "    data = np.zeros((len(df), n_levels), dtype=\"float32\")\n",
    "    for i in range(n_levels):\n",
    "        col = f\"{prefix}L{i:02d}\"\n",
    "        if col in df.columns:\n",
    "            data[:, i] = df[col].values\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a314f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ts_features_from_profiles(profiles):\n",
    "    \"\"\"\n",
    "    profiles: (T, Z, F) mit F = len(PROFILE_PREFIXES)\n",
    "    Level 50=unten → Level 0=oben\n",
    "    \"\"\"\n",
    "\n",
    "    # --- hard-coded z-levels (physikalische Höhen) ---\n",
    "    z = np.array([\n",
    "        3.136780e+04, 2.736595e+04, 2.492369e+04, 2.294698e+04, 2.125334e+04,\n",
    "        1.975951e+04, 1.841803e+04, 1.719845e+04, 1.607970e+04, 1.504641e+04,\n",
    "        1.408694e+04, 1.319217e+04, 1.235481e+04, 1.156892e+04, 1.082956e+04,\n",
    "        1.013258e+04, 9.474455e+03, 8.852140e+03, 8.263009e+03, 7.704765e+03,\n",
    "        7.175387e+03, 6.673090e+03, 6.196285e+03, 5.743555e+03, 5.313629e+03,\n",
    "        4.905366e+03, 4.517735e+03, 4.149806e+03, 3.800737e+03, 3.469765e+03,\n",
    "        3.156199e+03, 2.859414e+03, 2.578843e+03, 2.313976e+03, 2.064356e+03,\n",
    "        1.829575e+03, 1.609273e+03, 1.403137e+03, 1.210904e+03, 1.032357e+03,\n",
    "        8.673333e+02, 7.157275e+02, 5.774996e+02, 4.526887e+02, 3.414336e+02,\n",
    "        2.440081e+02, 1.608839e+02, 9.285786e+01, 4.137239e+01, 1.000000e+01\n",
    "    ], dtype=\"float32\")\n",
    "    \n",
    "    idx = {name: i for i, name in enumerate(PROFILE_PREFIXES)}\n",
    "    \n",
    "    T, Z = profiles.shape[:2]\n",
    "    if Z != z.shape[0]:\n",
    "        raise ValueError(f\"Z={Z} passt nicht zu z_levels={z.shape[0]}\")\n",
    "    \n",
    "    levels = np.arange(Z-1, -1, -1, dtype=\"float32\")  # 49...0 (unten→oben)\n",
    "    \n",
    "    ts_features = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Profile extrahieren\n",
    "        qc_t = profiles[t, :, idx[\"qc_\"]]  # (50,)\n",
    "        qi_t = profiles[t, :, idx[\"qi_\"]]\n",
    "        qr_t = profiles[t, :, idx[\"qr_\"]]\n",
    "        w_t  = profiles[t, :, idx[\"w_\"]]\n",
    "        \n",
    "        # Wolkenmaske (qc+qi > Schwellwert)\n",
    "        cloud_mask = (qc_t + qi_t) > 0\n",
    "        \n",
    "        if not np.any(cloud_mask):\n",
    "            ts_features.append(np.zeros(12, dtype=\"float32\"))\n",
    "            continue\n",
    "        \n",
    "        # 1. GEOMETRIE\n",
    "        cloud_heights = z[cloud_mask]       # physikalische Höhen der Wolkenlevel [m]\n",
    "        cloud_base    = cloud_heights.min() # unterste Wolkenhöhe (kleinste z-Koordinate)\n",
    "        cloud_top     = cloud_heights.max() # oberste Wolkenhöhe (größte z-Koordinate)\n",
    "        cloud_thickness = cloud_top - cloud_base\n",
    "        # Hinweis: z[0] = 31367m (oben), z[49] = 10m (unten)\n",
    "        \n",
    "        # 2. MASSE\n",
    "        cloud_mass = np.sum(qc_t + qi_t)\n",
    "        rain_mass  = np.sum(qr_t)\n",
    "        \n",
    "        # 3. DYNAMIK (nur in Wolke)\n",
    "        w_in_cloud = w_t[cloud_mask]\n",
    "        mean_w = np.mean(w_in_cloud)\n",
    "        max_w  = np.max(w_in_cloud)\n",
    "        \n",
    "        # 4. HÖHENINFO (Level-Indizes 0-49)\n",
    "        height_max_qc = float(z[int(np.argmax(qc_t))])\n",
    "        height_max_w  = float(z[int(np.argmax(w_t))])\n",
    "        \n",
    "        # 5. EXTRA (wie in deinem Code)\n",
    "        weights = qc_t + qi_t\n",
    "        center_of_mass = float(np.sum(z * weights) / (cloud_mass + 1e-12))\n",
    "        max_qc = np.max(qc_t)\n",
    "        std_w_cloud = np.std(w_in_cloud)\n",
    "        \n",
    "        ts_features.append([\n",
    "            cloud_base, cloud_top, cloud_thickness,      # Geometrie (3)\n",
    "            cloud_mass, rain_mass,                       # Masse (2)  \n",
    "            mean_w, max_w,                               # Dynamik (2)\n",
    "            height_max_qc, height_max_w,                 # Höheninfo (2)\n",
    "            center_of_mass, max_qc, std_w_cloud          # Extra (3)\n",
    "        ])\n",
    "    \n",
    "    return np.array(ts_features, dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9539318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cloud(df):\n",
    "    df = df.sort_values(\"time\")\n",
    "\n",
    "    # Target\n",
    "    y_lifetime = compute_remaining_lifetime(df)\n",
    "\n",
    "    # Profile (T, Z, F)\n",
    "    profile_features = []\n",
    "    for prefix in PROFILE_PREFIXES:\n",
    "        prof = extract_profile(df, prefix, n_levels=50)\n",
    "        profile_features.append(prof)\n",
    "    \n",
    "    profiles = np.stack(profile_features, axis=-1)\n",
    "\n",
    "    # cin and cape only every 5min interpolate the rest\n",
    "    df['cin_ml_L00'] = df['cin_ml_L00'].interpolate(method='linear').bfill()\n",
    "    df['cape_ml_L00'] = df['cape_ml_L00'].interpolate(method='linear').bfill()\n",
    "    \n",
    "    # Scalars\n",
    "    scalars = df[SCALAR_FEATURES].values.astype(\"float32\")\n",
    "    \n",
    "    # neue TS-Features\n",
    "    ts_features = extract_ts_features_from_profiles(profiles)\n",
    "\n",
    "    return {\n",
    "        \"ts_features\": ts_features,      # (T, 12)\n",
    "        \"scalars\": scalars,              # (T, 8)\n",
    "        \"y\": np.array(y_lifetime, dtype=\"float32\")[:, None]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "045a4afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(files):\n",
    "    samples = []\n",
    "    for f in files:\n",
    "        local_file = hf_hub_download(\n",
    "            repo_id=REPO_ID,\n",
    "            repo_type=\"dataset\",\n",
    "            filename=f,\n",
    "        )\n",
    "        df = pd.read_csv(local_file)\n",
    "        \n",
    "        if len(df) <= CUTOFF_STEPS:\n",
    "            continue\n",
    "        \n",
    "        sample = preprocess_cloud(df)\n",
    "        samples.append(sample)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52f88e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded: 150 train, 25 val, 25 test\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "train_samples = load_and_preprocess(train_files)\n",
    "val_samples   = load_and_preprocess(val_files)\n",
    "test_samples  = load_and_preprocess(test_files)\n",
    "\n",
    "print(f\"Loaded: {len(train_samples)} train, {len(val_samples)} val, {len(test_samples)} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41be9837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window: 60 Steps (30 min) | Stride: 6 Steps (3.0 min) | → 3984 Sequenzen\n",
      "  Window: 60 Steps (30 min) | Stride: 12 Steps (6.0 min) | → 316 Sequenzen\n",
      "  Window: 60 Steps (30 min) | Stride: 60 Steps (30.0 min) | → 80 Sequenzen\n"
     ]
    }
   ],
   "source": [
    "def create_fixed_window_sequences(samples, \n",
    "                                   window_minutes=WINDOW_MINUTES,\n",
    "                                   stride=1):\n",
    "    \"\"\"\n",
    "    Erstellt Sliding-Window Sequenzen aus den Samples.\n",
    "    \n",
    "    Parameter:\n",
    "    ----------\n",
    "    samples      : Liste von preprocessierten Wolken-Dicts\n",
    "    window_minutes: Fensterlänge in Minuten\n",
    "    stride       : Schrittweite des Fensters in Zeitschritten (30-Sek.-Steps)\n",
    "                   stride=1  → maximale Überlappung (viele, korrelierte Samples)\n",
    "                   stride=30 → kein Overlap (1 Fenster pro 15 Minuten)\n",
    "                   stride=WINDOW_STEPS → kein Overlap (non-overlapping windows)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : np.array, Shape (N, WINDOW_STEPS, n_features)\n",
    "    y : np.array, Shape (N, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 30 Sek./Step → 30min = 60 Steps\n",
    "    WINDOW_STEPS = int(window_minutes * 60 / 30)\n",
    "    \n",
    "    X, y = [], []\n",
    "    skipped = 0\n",
    "\n",
    "    for s in samples:\n",
    "        combined = np.concatenate([s[\"scalars\"], s[\"ts_features\"]], axis=1)\n",
    "        T = len(combined)\n",
    "        \n",
    "        # Wolke zu kurz für auch nur ein Fenster\n",
    "        if T <= WINDOW_STEPS:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Sliding Window mit Stride\n",
    "        for i in range(0, T - WINDOW_STEPS, stride):\n",
    "            X.append(combined[i : i + WINDOW_STEPS])\n",
    "            y.append(s[\"y\"][i + WINDOW_STEPS])\n",
    "    \n",
    "    if skipped > 0:\n",
    "        print(f\"  ⚠️  {skipped} Wolken zu kurz für Window ({WINDOW_STEPS} Steps), übersprungen.\")\n",
    "    \n",
    "    X = np.array(X, dtype=\"float32\")\n",
    "    y = np.array(y, dtype=\"float32\")\n",
    "    \n",
    "    print(f\"  Window: {WINDOW_STEPS} Steps ({window_minutes} min) | \"\n",
    "          f\"Stride: {stride} Steps ({stride*30/60:.1f} min) | \"\n",
    "          f\"→ {X.shape[0]} Sequenzen\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Training: moderater Stride für mehr Daten, aber weniger Korrelation\n",
    "X_train, y_train = create_fixed_window_sequences(train_samples, stride=6)\n",
    "\n",
    "# Validation/Test: größerer Stride → unabhängigere Evaluation\n",
    "X_val, y_val   = create_fixed_window_sequences(val_samples,   stride=12)\n",
    "X_test, y_test = create_fixed_window_sequences(test_samples,  stride=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9300d26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3984, 60, 20)\n",
      "y_train shape: (3984, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")  # Sollte (N, 60, 20) sein\n",
    "print(f\"y_train shape: {y_train.shape}\")  # Sollte (N, 1) sein\n",
    "assert X_train.shape[2] == len(SCALAR_FEATURES) + 12  # 8 + 12 = 20 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af561cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloud_base  > cloud_top? False\n",
      "thickness > 0? True\n",
      "Sample values:\n",
      " [[3.4143359e+02 2.1253340e+04 2.0911906e+04 8.6598527e-03 1.0412185e-02]\n",
      " [3.4143359e+02 2.1253340e+04 2.0911906e+04 9.4145974e-03 1.1317367e-02]\n",
      " [3.4143359e+02 2.1253340e+04 2.0911906e+04 1.0146300e-02 1.2264996e-02]]\n"
     ]
    }
   ],
   "source": [
    "sample = train_samples[0]\n",
    "ts_features = sample[\"ts_features\"]\n",
    "\n",
    "print(\"cloud_base  > cloud_top?\", np.all(ts_features[:,0] >= ts_features[:,1]))\n",
    "print(\"thickness > 0?\", np.all(ts_features[:,2] > 0))\n",
    "print(\"Sample values:\\n\", ts_features[:3, :5])  # Erste 5 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6f62e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-2ed7babb-3085-43f5-b9ed-7846726f2776\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cape_ml_L00</th>\n",
       "      <th>cin_ml_L00</th>\n",
       "      <th>lwp_L00</th>\n",
       "      <th>iwp_L00</th>\n",
       "      <th>rain_gsp_rate_L00</th>\n",
       "      <th>tqc_L00</th>\n",
       "      <th>tqi_L00</th>\n",
       "      <th>area_m2</th>\n",
       "      <th>cloud_base</th>\n",
       "      <th>cloud_top</th>\n",
       "      <th>cloud_thickness</th>\n",
       "      <th>cloud_mass</th>\n",
       "      <th>rain_mass</th>\n",
       "      <th>mean_w</th>\n",
       "      <th>max_w</th>\n",
       "      <th>height_max_qc</th>\n",
       "      <th>height_max_w</th>\n",
       "      <th>center_of_mass</th>\n",
       "      <th>max_qc</th>\n",
       "      <th>std_w_in_cloud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>604.720398</td>\n",
       "      <td>1.058137</td>\n",
       "      <td>2.507058</td>\n",
       "      <td>0.188360</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>2.034199</td>\n",
       "      <td>0.037669</td>\n",
       "      <td>31360000.0</td>\n",
       "      <td>715.727478</td>\n",
       "      <td>21253.339844</td>\n",
       "      <td>20537.613281</td>\n",
       "      <td>0.008732</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.233418</td>\n",
       "      <td>1.020461</td>\n",
       "      <td>3156.198975</td>\n",
       "      <td>1609.272949</td>\n",
       "      <td>2483.903564</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.409249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>604.720398</td>\n",
       "      <td>1.058137</td>\n",
       "      <td>2.567386</td>\n",
       "      <td>0.176616</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>2.066459</td>\n",
       "      <td>0.037015</td>\n",
       "      <td>31360000.0</td>\n",
       "      <td>577.499573</td>\n",
       "      <td>21253.339844</td>\n",
       "      <td>20675.839844</td>\n",
       "      <td>0.008874</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.242999</td>\n",
       "      <td>1.030474</td>\n",
       "      <td>3156.198975</td>\n",
       "      <td>1609.272949</td>\n",
       "      <td>2476.058594</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.409450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>604.720398</td>\n",
       "      <td>1.058137</td>\n",
       "      <td>2.617499</td>\n",
       "      <td>0.166323</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>2.089409</td>\n",
       "      <td>0.036390</td>\n",
       "      <td>31360000.0</td>\n",
       "      <td>577.499573</td>\n",
       "      <td>21253.339844</td>\n",
       "      <td>20675.839844</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>0.246419</td>\n",
       "      <td>1.034724</td>\n",
       "      <td>3156.198975</td>\n",
       "      <td>1609.272949</td>\n",
       "      <td>2468.230713</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.412110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>604.720398</td>\n",
       "      <td>1.058137</td>\n",
       "      <td>2.656605</td>\n",
       "      <td>0.157891</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>2.102927</td>\n",
       "      <td>0.035792</td>\n",
       "      <td>31360000.0</td>\n",
       "      <td>577.499573</td>\n",
       "      <td>21253.339844</td>\n",
       "      <td>20675.839844</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.248115</td>\n",
       "      <td>1.033336</td>\n",
       "      <td>3156.198975</td>\n",
       "      <td>1609.272949</td>\n",
       "      <td>2461.224854</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.412336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>604.720398</td>\n",
       "      <td>1.058137</td>\n",
       "      <td>2.683506</td>\n",
       "      <td>0.151085</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>2.106670</td>\n",
       "      <td>0.035219</td>\n",
       "      <td>70560000.0</td>\n",
       "      <td>577.499573</td>\n",
       "      <td>21253.339844</td>\n",
       "      <td>20675.839844</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>1.024991</td>\n",
       "      <td>3156.198975</td>\n",
       "      <td>1609.272949</td>\n",
       "      <td>2455.364258</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.409315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ed7babb-3085-43f5-b9ed-7846726f2776')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-2ed7babb-3085-43f5-b9ed-7846726f2776 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-2ed7babb-3085-43f5-b9ed-7846726f2776');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   cape_ml_L00  cin_ml_L00   lwp_L00   iwp_L00  rain_gsp_rate_L00   tqc_L00  \\\n",
       "0   604.720398    1.058137  2.507058  0.188360           0.000228  2.034199   \n",
       "1   604.720398    1.058137  2.567386  0.176616           0.000240  2.066459   \n",
       "2   604.720398    1.058137  2.617499  0.166323           0.000243  2.089409   \n",
       "3   604.720398    1.058137  2.656605  0.157891           0.000245  2.102927   \n",
       "4   604.720398    1.058137  2.683506  0.151085           0.000247  2.106670   \n",
       "\n",
       "    tqi_L00     area_m2  cloud_base     cloud_top  cloud_thickness  \\\n",
       "0  0.037669  31360000.0  715.727478  21253.339844     20537.613281   \n",
       "1  0.037015  31360000.0  577.499573  21253.339844     20675.839844   \n",
       "2  0.036390  31360000.0  577.499573  21253.339844     20675.839844   \n",
       "3  0.035792  31360000.0  577.499573  21253.339844     20675.839844   \n",
       "4  0.035219  70560000.0  577.499573  21253.339844     20675.839844   \n",
       "\n",
       "   cloud_mass  rain_mass    mean_w     max_w  height_max_qc  height_max_w  \\\n",
       "0    0.008732   0.002572  0.233418  1.020461    3156.198975   1609.272949   \n",
       "1    0.008874   0.002705  0.242999  1.030474    3156.198975   1609.272949   \n",
       "2    0.008977   0.002832  0.246419  1.034724    3156.198975   1609.272949   \n",
       "3    0.009041   0.002950  0.248115  1.033336    3156.198975   1609.272949   \n",
       "4    0.009063   0.003056  0.247676  1.024991    3156.198975   1609.272949   \n",
       "\n",
       "   center_of_mass    max_qc  std_w_in_cloud  \n",
       "0     2483.903564  0.001669        0.409249  \n",
       "1     2476.058594  0.001720        0.409450  \n",
       "2     2468.230713  0.001759        0.412110  \n",
       "3     2461.224854  0.001787        0.412336  \n",
       "4     2455.364258  0.001802        0.409315  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_samples[8]\n",
    "\n",
    "scalars = sample[\"scalars\"]          # (T, 8)\n",
    "ts      = sample[\"ts_features\"]      # (T, 12)\n",
    "\n",
    "# Zu (T, 20) zusammenfügen\n",
    "combined = np.concatenate([scalars, ts], axis=1)\n",
    "\n",
    "cols = [\n",
    "    \"cape_ml_L00\", \"cin_ml_L00\",\n",
    "    \"lwp_L00\",\n",
    "    \"iwp_L00\", \"rain_gsp_rate_L00\",\n",
    "    \"tqc_L00\", \"tqi_L00\", \"area_m2\",\n",
    "    \"cloud_base\", \"cloud_top\", \"cloud_thickness\",\n",
    "    \"cloud_mass\", \"rain_mass\",\n",
    "    \"mean_w\", \"max_w\",\n",
    "    \"height_max_qc\", \"height_max_w\",\n",
    "    \"center_of_mass\", \"max_qc\", \"std_w_in_cloud\"\n",
    "]\n",
    "\n",
    "ts_df = pd.DataFrame(combined, columns=cols)\n",
    "ts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5ef75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1024cb92",
   "metadata": {},
   "source": [
    "### Normalisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3a3028a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Normalization\n",
      "TS alle Splits skaliert\n",
      "Final shapes:\n",
      "  Train: TS=(3984, 60, 20)\n",
      "  Val:   TS=(316, 60, 20)\n",
      "  Test:  TS=(80, 60, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Normalization\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_flat = X_train.reshape(-1, X_train.shape[-1])  # (4500, 12)\n",
    "X_train_scaled = scaler.fit_transform(X_train_flat).reshape(X_train.shape)\n",
    "\n",
    "X_val_flat = X_val.reshape(-1, X_val.shape[-1])\n",
    "X_val_scaled = scaler.transform(X_val_flat).reshape(X_val.shape)\n",
    "\n",
    "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
    "X_test_scaled = scaler.transform(X_test_flat).reshape(X_test.shape)\n",
    "\n",
    "print(\"TS alle Splits skaliert\")\n",
    "\n",
    "print(f\"Final shapes:\")\n",
    "print(f\"  Train: TS={X_train_scaled.shape}\")\n",
    "print(f\"  Val:   TS={X_val_scaled.shape}\")\n",
    "print(f\"  Test:  TS={X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e27aca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shapes:\n",
      "  Train: TS=(3984, 1)\n",
      "  Val:   TS=(316, 1)\n",
      "  Test:  TS=(80, 1)\n"
     ]
    }
   ],
   "source": [
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_val_scaled   = y_scaler.transform(y_val)\n",
    "y_test_scaled  = y_scaler.transform(y_test)\n",
    "\n",
    "print(f\"Final shapes:\")\n",
    "print(f\"  Train: TS={y_train_scaled.shape}\")\n",
    "print(f\"  Val:   TS={y_val_scaled.shape}\")\n",
    "print(f\"  Test:  TS={y_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50e9ccb",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "[Discuss any hyperparameter tuning methods you've applied, such as Grid Search or Random Search, and the rationale behind them.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1371d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement hyperparameter tuning\n",
    "# Example using GridSearchCV with a DecisionTreeClassifier\n",
    "# param_grid = {'max_depth': [2, 4, 6, 8]}\n",
    "# grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160704d6",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "[Implement the final model(s) you've selected based on the above steps.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8e5507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the final model(s)\n",
    "# Example: model = YourChosenModel(best_hyperparameters)\n",
    "# model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60d844",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "[Clearly specify which metrics you'll use to evaluate the model performance, and why you've chosen these metrics.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "268c0d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using your chosen metrics\n",
    "# Example for classification\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example for regression\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Your evaluation code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c874306",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "[Compare the performance of your model(s) against the baseline model. Discuss any improvements or setbacks and the reasons behind them.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c4da1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative Analysis code (if applicable)\n",
    "# Example: comparing accuracy of the baseline model and the new model\n",
    "# print(f\"Baseline Model Accuracy: {baseline_accuracy}, New Model Accuracy: {new_model_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
