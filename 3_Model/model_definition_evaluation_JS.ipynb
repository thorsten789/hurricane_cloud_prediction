{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as pl\n",
    "import os, json, hashlib\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from huggingface_hub import login, list_repo_files, hf_hub_download, upload_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ID = \"mttfst/Paulette_Cloud_Tracks\"\n",
    "token = \"\"\n",
    "\n",
    "WINDOW_MINUTES =  30 #Minuten\n",
    "CUTOFF_STEPS = 5\n",
    "\n",
    "MODEL = \"SimpleRNN\"\n",
    "LOSS = \"MSE\"\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 2 #40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Login HuggingFace\n",
    "# =========================================\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_ID: simple_rnn_2026-02-15_00-39-37_a7afa260af\n",
      "[HF] uploaded: runs/simple_rnn_2026-02-15_00-39-37_a7afa260af/setup.json\n"
     ]
    }
   ],
   "source": [
    "# Du kannst hier ein separates Repo für Logs/Configs nutzen (empfohlen),\n",
    "# oder du lässt es auf dem Dataset-Repo laufen.\n",
    "CONFIG_REPO_ID = REPO_ID  # z.B. \"thorsten789/hurricane_cloud_runs\"\n",
    "\n",
    "def make_run_id(prefix: str, config: dict) -> str:\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    cfg_str = json.dumps(config, sort_keys=True)\n",
    "    h = hashlib.sha1(cfg_str.encode(\"utf-8\")).hexdigest()[:10]\n",
    "    return f\"{prefix}_{ts}_{h}\"\n",
    "\n",
    "def save_json_local(path: str, data: dict) -> str:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    return path\n",
    "\n",
    "def upload_json_hf(local_path: str, run_id: str, name: str, base_dir: str = \"runs\"):\n",
    "    \"\"\"Lädt JSON als {base_dir}/{run_id}/{name}.json in CONFIG_REPO_ID hoch.\"\"\"\n",
    "    try:\n",
    "        path_in_repo = f\"{base_dir}/{run_id}/{name}.json\"\n",
    "        upload_file(\n",
    "            path_or_fileobj=local_path,\n",
    "            path_in_repo=path_in_repo,\n",
    "            repo_id=CONFIG_REPO_ID,\n",
    "            repo_type=\"dataset\",\n",
    "            commit_message=f\"Add {name}.json for {run_id}\",\n",
    "        )\n",
    "        print(f\"[HF] uploaded: {path_in_repo}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[HF] upload skipped/failed ({name}): {e}\")\n",
    "\n",
    "# --- zentrale Run-Config (die ID basiert auf config -> sinnvoller Run-Name)\n",
    "RUN_CONFIG = {\n",
    "    \"model\": MODEL,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"units1\": 64,\n",
    "    \"units2\": 32,\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"cutoff_steps\": CUTOFF_STEPS,\n",
    "    \"loss\": LOSS,\n",
    "}\n",
    "\n",
    "RUN_ID = make_run_id(\"simple_rnn\", RUN_CONFIG)\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "\n",
    "# Setup sofort speichern (damit du schon am Anfang einen Run hast)\n",
    "setup = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"config\": RUN_CONFIG,\n",
    "    \"data\": {\"repo_id\": REPO_ID},\n",
    "    \"meta\": {\"notebook\": \"3_Model/model_definition_evaluation_JS.ipynb\"},\n",
    "}\n",
    "\n",
    "# +++ Logging: Save Setup +++\n",
    "local_setup = save_json_local(f\"runs_local/{RUN_ID}/setup.json\", setup)\n",
    "upload_json_hf(local_setup, RUN_ID, \"setup\")\n",
    "\n",
    "class AutoSaveTrain(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Speichert train.json am Ende von model.fit (History + best_val_loss).\"\"\"\n",
    "    def __init__(self, run_id: str):\n",
    "        super().__init__()\n",
    "        self.run_id = run_id\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        hist = getattr(self.model, \"history\", None)\n",
    "        history_dict = hist.history if hist is not None else {}\n",
    "\n",
    "        train_data = {\n",
    "            \"run_id\": self.run_id,\n",
    "            \"history\": history_dict,\n",
    "            \"summary\": {\n",
    "                \"best_val_loss\": float(min(history_dict[\"val_loss\"])) if \"val_loss\" in history_dict else None,\n",
    "                \"final_train_loss\": float(history_dict[\"loss\"][-1]) if \"loss\" in history_dict and len(history_dict[\"loss\"]) else None,\n",
    "            },\n",
    "        }\n",
    "        local_train = save_json_local(f\"runs_local/{self.run_id}/train.json\", train_data)\n",
    "        upload_json_hf(local_train, self.run_id, \"train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from HuggingFace\n",
    "\n",
    "TRACK LENGTH FILE:\n",
    "\n",
    "Diese CSV enthält Metadaten über alle Wolken:\n",
    "- filename: Dateiname des Tracks\n",
    "- track_len: Anzahl Zeitschritte (Lebensdauer)\n",
    "\n",
    "WARUM WICHTIG?\n",
    "- Wir trainieren nur auf Wolken mit >= 120 Zeitschritten\n",
    "- Grund: Kurze Wolken (<60 Minuten) sind zu variabel/chaotisch\n",
    "- Längere Wolken zeigen klare Lebenszyklen\n",
    "\n",
    "DATENQUELLE:\n",
    "Diese Datei wurde vorberechnet aus allen Tracks.\n",
    "Spart Zeit beim Training-Setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️  Downloading from HF: mttfst/Paulette_Cloud_Tracks/track_len/track_len_exp_1.1.csv\n",
      "✅ Loading track_len from HF-cached file: /root/.cache/huggingface/hub/datasets--mttfst--Paulette_Cloud_Tracks/snapshots/8248ff1af359eabb4055f7d98300dd3810e359c3/track_len/track_len_exp_1.1.csv\n",
      "tracks_120 shape: (9227, 2)\n",
      "Total CSV tracks with at least 120 timesteps: 1115\n",
      "Train: 150, Val: 25, Test: 25\n"
     ]
    }
   ],
   "source": [
    "tracks_120 = None\n",
    "\n",
    "# Wenn lokal nicht da: von Hugging Face Dataset-Repo herunterladen\n",
    "if tracks_120 is None:\n",
    "    from huggingface_hub import hf_hub_download\n",
    "\n",
    "    filename_in_repo = \"track_len/track_len_exp_1.1.csv\"\n",
    "\n",
    "    print(f\"⬇️  Downloading from HF: {REPO_ID}/{filename_in_repo}\")\n",
    "    local_file = hf_hub_download(\n",
    "        repo_id=REPO_ID,\n",
    "        repo_type=\"dataset\",\n",
    "        filename=filename_in_repo,\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Loading track_len from HF-cached file: {local_file}\")\n",
    "    tracks_120 = pd.read_csv(local_file)\n",
    "\n",
    "print(\"tracks_120 shape:\", tracks_120.shape)\n",
    "tracks_120.head()\n",
    "\n",
    "# =========================================\n",
    "# Dataset-Split\n",
    "# =========================================\n",
    "files = list_repo_files(REPO_ID, repo_type=\"dataset\")\n",
    "csv_files = [f for f in files if f.startswith(\"exp_1.1/\") and f.endswith(\".csv\")]\n",
    "\n",
    "tracks_120 = tracks_120[tracks_120.track_len >= 120]\n",
    "print(\"Total CSV tracks with at least 120 timesteps:\", len(tracks_120))\n",
    "\n",
    "tracks_120 = tracks_120.filename.to_list()\n",
    "\n",
    "csv_files = [\n",
    "    f for f in csv_files\n",
    "    if f.split(\"/\")[1] in tracks_120\n",
    "]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "n = len(csv_files)\n",
    "#train_files = csv_files[: int(0.7 * n)]\n",
    "#val_files   = csv_files[int(0.7 * n): int(0.85 * n)]\n",
    "#test_files  = csv_files[int(0.85 * n):]\n",
    "\n",
    "train_files= csv_files[:150]\n",
    "val_files= csv_files[100:125]\n",
    "test_files= csv_files[150:175]\n",
    "\n",
    "print(f\"Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "[Discuss the type(s) of models you consider for this task, and justify the selection.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "[Describe any additional feature engineering you've performed beyond what was done for the baseline model.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILE_PREFIXES = [\"qr_\", \"qc_\", \"qi_\", \"qs_\", \"qg_\", \"qv_\", \"roh_\", \"w_\"]\n",
    "\n",
    "SCALAR_FEATURES = [\n",
    "    \"cape_ml_L00\", \"cin_ml_L00\",\n",
    "    \"lwp_L00\",\n",
    "    \"iwp_L00\", \"rain_gsp_rate_L00\",\n",
    "    \"tqc_L00\", \"tqi_L00\", \"area_m2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_remaining_lifetime(df, timestep_minutes=5):\n",
    "    \"\"\"\n",
    "    Berechnet verbleibende Lebensdauer pro Zeitschritt\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    return [(n - i - 1) * timestep_minutes for i in range(n)]\n",
    "\n",
    "#def compute_future_rain(df, timestep_minutes=5):\n",
    "#    rain = df[\"rain_gsp_rate_L00\"].values\n",
    "#    dt = timestep_minutes * 60  # Sekunden\n",
    "\n",
    "#    future_rain = []\n",
    "#    for i in range(len(rain)):\n",
    "#        future_rain.append(rain[i:].sum() * dt)\n",
    "\n",
    "#    return future_rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_profile(df, prefix, n_levels=50):\n",
    "    \"\"\"\n",
    "    Extrahiert ein vertikales Profil mit exakt n_levels.\n",
    "    Fehlende Level werden mit 0 aufgefüllt.\n",
    "    \"\"\"\n",
    "    data = np.zeros((len(df), n_levels), dtype=\"float32\")\n",
    "    for i in range(n_levels):\n",
    "        col = f\"{prefix}L{i:02d}\"\n",
    "        if col in df.columns:\n",
    "            data[:, i] = df[col].values\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ts_features_from_profiles(profiles):\n",
    "    \"\"\"\n",
    "    profiles: (T, Z, F) mit F = len(PROFILE_PREFIXES)\n",
    "    Level 50=unten → Level 0=oben\n",
    "    \"\"\"\n",
    "    idx = {name: i for i, name in enumerate(PROFILE_PREFIXES)}\n",
    "    \n",
    "    T, Z = profiles.shape[:2]\n",
    "    levels = np.arange(Z-1, -1, -1, dtype=\"float32\")  # 49...0 (unten→oben)\n",
    "    \n",
    "    ts_features = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Profile extrahieren\n",
    "        qc_t = profiles[t, :, idx[\"qc_\"]]  # (50,)\n",
    "        qi_t = profiles[t, :, idx[\"qi_\"]]\n",
    "        qr_t = profiles[t, :, idx[\"qr_\"]]\n",
    "        w_t  = profiles[t, :, idx[\"w_\"]]\n",
    "        \n",
    "        # Wolkenmaske (qc+qi > Schwellwert)\n",
    "        cloud_mask = (qc_t + qi_t) > 1e-10\n",
    "        \n",
    "        if not np.any(cloud_mask):\n",
    "            ts_features.append(np.zeros(12, dtype=\"float32\"))\n",
    "            continue\n",
    "        \n",
    "        # 1. GEOMETRIE\n",
    "        cloud_levels = levels[cloud_mask]\n",
    "        cloud_base   = cloud_levels.max()   # höchster Level = unten\n",
    "        cloud_top    = cloud_levels.min()   # niedrigster Level = oben\n",
    "        cloud_thickness = cloud_base - cloud_top\n",
    "        \n",
    "        # 2. MASSE\n",
    "        cloud_mass = np.sum(qc_t + qi_t)\n",
    "        rain_mass  = np.sum(qr_t)\n",
    "        \n",
    "        # 3. DYNAMIK (nur in Wolke)\n",
    "        w_in_cloud = w_t[cloud_mask]\n",
    "        mean_w = np.mean(w_in_cloud)\n",
    "        max_w  = np.max(w_in_cloud)\n",
    "        \n",
    "        # 4. HÖHENINFO (Level-Indizes 0-49)\n",
    "        height_max_qc = np.argmax(qc_t)\n",
    "        height_max_w  = np.argmax(w_t)\n",
    "        \n",
    "        # 5. EXTRA (wie in deinem Code)\n",
    "        center_of_mass = np.sum(levels * (qc_t + qi_t)) / max(cloud_mass, 1e-12)\n",
    "        max_qc = np.max(qc_t)\n",
    "        std_w_cloud = np.std(w_in_cloud)\n",
    "        \n",
    "        ts_features.append([\n",
    "            cloud_base, cloud_top, cloud_thickness,      # Geometrie (3)\n",
    "            cloud_mass, rain_mass,                       # Masse (2)  \n",
    "            mean_w, max_w,                               # Dynamik (2)\n",
    "            height_max_qc, height_max_w,                 # Höheninfo (2)\n",
    "            center_of_mass, max_qc, std_w_cloud          # Extra (3)\n",
    "        ])\n",
    "    \n",
    "    return np.array(ts_features, dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cloud(df):\n",
    "    df = df.sort_values(\"time\")\n",
    "\n",
    "    # Target\n",
    "    y_lifetime = compute_remaining_lifetime(df)\n",
    "\n",
    "    # Profile (T, Z, F)\n",
    "    profile_features = []\n",
    "    for prefix in PROFILE_PREFIXES:\n",
    "        prof = extract_profile(df, prefix, n_levels=50)\n",
    "        profile_features.append(prof)\n",
    "    \n",
    "    profiles = np.stack(profile_features, axis=-1)\n",
    "    \n",
    "    # Scalars\n",
    "    scalars = df[SCALAR_FEATURES].values.astype(\"float32\")\n",
    "    \n",
    "    # neue TS-Features\n",
    "    ts_features = extract_ts_features_from_profiles(profiles)\n",
    "\n",
    "    return {\n",
    "        \"ts_features\": ts_features,      # (T, 12)\n",
    "        \"scalars\": scalars,              # (T, 8)\n",
    "        \"y\": np.array(y_lifetime, dtype=\"float32\")[:, None]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(files):\n",
    "    samples = []\n",
    "    for f in files:\n",
    "        local_file = hf_hub_download(\n",
    "            repo_id=REPO_ID,\n",
    "            repo_type=\"dataset\",\n",
    "            filename=f,\n",
    "        )\n",
    "        df = pd.read_csv(local_file)\n",
    "        \n",
    "        if len(df) <= CUTOFF_STEPS:\n",
    "            continue\n",
    "        \n",
    "        sample = preprocess_cloud(df)\n",
    "        samples.append(sample)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded: 150 train, 25 val, 25 test\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "train_samples = load_and_preprocess(train_files)\n",
    "val_samples   = load_and_preprocess(val_files)\n",
    "test_samples  = load_and_preprocess(test_files)\n",
    "\n",
    "print(f\"Loaded: {len(train_samples)} train, {len(val_samples)} val, {len(test_samples)} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fixed_window_sequences(samples, window_steps=WINDOW_MINUTES):  # 30min\n",
    "    X, y = [], []\n",
    "    WINDOW_STEPS = int(WINDOW_MINUTES * 60 / 30)  # 30min * 60s/min / 30s/step = 60\n",
    "\n",
    "    for s in samples:\n",
    "        combined = np.concatenate([s[\"scalars\"], s[\"ts_features\"]], axis=1)\n",
    "        for i in range(len(combined) - WINDOW_STEPS):\n",
    "            X.append(combined[i:i+WINDOW_STEPS])  # (60, 20)\n",
    "            y.append(s[\"y\"][i+WINDOW_STEPS])  # RUL am Ende\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_fixed_window_sequences(train_samples, WINDOW_MINUTES)\n",
    "X_val, y_val = create_fixed_window_sequences(val_samples, WINDOW_MINUTES)\n",
    "X_test, y_test = create_fixed_window_sequences(test_samples, WINDOW_MINUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloud_base  > cloud_top? True\n",
      "thickness > 0? True\n",
      "Sample values:\n",
      " [[4.2000000e+01 5.0000000e+00 3.7000000e+01 8.6598527e-03 1.0412185e-02]\n",
      " [4.2000000e+01 5.0000000e+00 3.7000000e+01 9.4145974e-03 1.1317367e-02]\n",
      " [4.2000000e+01 5.0000000e+00 3.7000000e+01 1.0146300e-02 1.2264996e-02]]\n"
     ]
    }
   ],
   "source": [
    "sample = train_samples[0]\n",
    "ts_features = sample[\"ts_features\"]\n",
    "\n",
    "print(\"cloud_base  > cloud_top?\", np.all(ts_features[:,0] >= ts_features[:,1]))\n",
    "print(\"thickness > 0?\", np.all(ts_features[:,2] > 0))\n",
    "print(\"Sample values:\\n\", ts_features[:3, :5])  # Erste 5 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-19178cc1-ea54-4930-a775-15a4d29cc673\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cape_ml_L00</th>\n",
       "      <th>cin_ml_L00</th>\n",
       "      <th>lwp_L00</th>\n",
       "      <th>iwp_L00</th>\n",
       "      <th>rain_gsp_rate_L00</th>\n",
       "      <th>tqc_L00</th>\n",
       "      <th>tqi_L00</th>\n",
       "      <th>area_m2</th>\n",
       "      <th>cloud_base</th>\n",
       "      <th>cloud_top</th>\n",
       "      <th>cloud_thickness</th>\n",
       "      <th>cloud_mass</th>\n",
       "      <th>rain_mass</th>\n",
       "      <th>mean_w</th>\n",
       "      <th>max_w</th>\n",
       "      <th>height_max_qc</th>\n",
       "      <th>height_max_w</th>\n",
       "      <th>center_of_mass</th>\n",
       "      <th>max_qc</th>\n",
       "      <th>std_w_in_cloud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.507058</td>\n",
       "      <td>0.188360</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>2.034199</td>\n",
       "      <td>0.037669</td>\n",
       "      <td>31360000.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.008732</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.271322</td>\n",
       "      <td>1.020461</td>\n",
       "      <td>30.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.148663</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.420280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.567386</td>\n",
       "      <td>0.176616</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>2.066459</td>\n",
       "      <td>0.037015</td>\n",
       "      <td>31360000.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.008874</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.281104</td>\n",
       "      <td>1.030474</td>\n",
       "      <td>30.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.120064</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.419132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.617499</td>\n",
       "      <td>0.166323</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>2.089409</td>\n",
       "      <td>0.036390</td>\n",
       "      <td>31360000.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>0.284971</td>\n",
       "      <td>1.034724</td>\n",
       "      <td>30.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.089861</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.421756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.656605</td>\n",
       "      <td>0.157891</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>2.102927</td>\n",
       "      <td>0.035792</td>\n",
       "      <td>31360000.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.286870</td>\n",
       "      <td>1.033336</td>\n",
       "      <td>30.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.061790</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.421885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.683506</td>\n",
       "      <td>0.151085</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>2.106670</td>\n",
       "      <td>0.035219</td>\n",
       "      <td>70560000.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.286429</td>\n",
       "      <td>1.024991</td>\n",
       "      <td>30.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.037300</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.418588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19178cc1-ea54-4930-a775-15a4d29cc673')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-19178cc1-ea54-4930-a775-15a4d29cc673 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-19178cc1-ea54-4930-a775-15a4d29cc673');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   cape_ml_L00  cin_ml_L00   lwp_L00   iwp_L00  rain_gsp_rate_L00   tqc_L00  \\\n",
       "0          NaN         NaN  2.507058  0.188360           0.000228  2.034199   \n",
       "1          NaN         NaN  2.567386  0.176616           0.000240  2.066459   \n",
       "2          NaN         NaN  2.617499  0.166323           0.000243  2.089409   \n",
       "3          NaN         NaN  2.656605  0.157891           0.000245  2.102927   \n",
       "4          NaN         NaN  2.683506  0.151085           0.000247  2.106670   \n",
       "\n",
       "    tqi_L00     area_m2  cloud_base  cloud_top  cloud_thickness  cloud_mass  \\\n",
       "0  0.037669  31360000.0        42.0        8.0             34.0    0.008732   \n",
       "1  0.037015  31360000.0        42.0        7.0             35.0    0.008874   \n",
       "2  0.036390  31360000.0        42.0        7.0             35.0    0.008977   \n",
       "3  0.035792  31360000.0        42.0        7.0             35.0    0.009041   \n",
       "4  0.035219  70560000.0        42.0        7.0             35.0    0.009063   \n",
       "\n",
       "   rain_mass    mean_w     max_w  height_max_qc  height_max_w  center_of_mass  \\\n",
       "0   0.002572  0.271322  1.020461           30.0          36.0       16.148663   \n",
       "1   0.002705  0.281104  1.030474           30.0          36.0       16.120064   \n",
       "2   0.002832  0.284971  1.034724           30.0          36.0       16.089861   \n",
       "3   0.002950  0.286870  1.033336           30.0          36.0       16.061790   \n",
       "4   0.003056  0.286429  1.024991           30.0          36.0       16.037300   \n",
       "\n",
       "     max_qc  std_w_in_cloud  \n",
       "0  0.001669        0.420280  \n",
       "1  0.001720        0.419132  \n",
       "2  0.001759        0.421756  \n",
       "3  0.001787        0.421885  \n",
       "4  0.001802        0.418588  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_samples[8]\n",
    "\n",
    "scalars = sample[\"scalars\"]          # (T, 8)\n",
    "ts      = sample[\"ts_features\"]      # (T, 12)\n",
    "\n",
    "# Zu (T, 20) zusammenfügen\n",
    "combined = np.concatenate([scalars, ts], axis=1)\n",
    "\n",
    "cols = [\n",
    "    \"cape_ml_L00\", \"cin_ml_L00\",\n",
    "    \"lwp_L00\",\n",
    "    \"iwp_L00\", \"rain_gsp_rate_L00\",\n",
    "    \"tqc_L00\", \"tqi_L00\", \"area_m2\",\n",
    "    \"cloud_base\", \"cloud_top\", \"cloud_thickness\",\n",
    "    \"cloud_mass\", \"rain_mass\",\n",
    "    \"mean_w\", \"max_w\",\n",
    "    \"height_max_qc\", \"height_max_w\",\n",
    "    \"center_of_mass\", \"max_qc\", \"std_w_in_cloud\"\n",
    "]\n",
    "\n",
    "ts_df = pd.DataFrame(combined, columns=cols)\n",
    "ts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Normalization\n",
      "TS alle Splits skaliert\n",
      "Final shapes:\n",
      "  Train: TS=(23528, 60, 20)\n",
      "  Val:   TS=(4580, 60, 20)\n",
      "  Test:  TS=(3646, 60, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Normalization\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_flat = X_train.reshape(-1, X_train.shape[-1])  # (4500, 12)\n",
    "X_train_scaled = scaler.fit_transform(X_train_flat).reshape(X_train.shape)\n",
    "\n",
    "X_val_flat = X_val.reshape(-1, X_val.shape[-1])\n",
    "X_val_scaled = scaler.transform(X_val_flat).reshape(X_val.shape)\n",
    "\n",
    "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
    "X_test_scaled = scaler.transform(X_test_flat).reshape(X_test.shape)\n",
    "\n",
    "print(\"TS alle Splits skaliert\")\n",
    "\n",
    "print(f\"Final shapes:\")\n",
    "print(f\"  Train: TS={X_train_scaled.shape}\")\n",
    "print(f\"  Val:   TS={X_val_scaled.shape}\")\n",
    "print(f\"  Test:  TS={X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "[Discuss any hyperparameter tuning methods you've applied, such as Grid Search or Random Search, and the rationale behind them.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement hyperparameter tuning\n",
    "# Example using GridSearchCV with a DecisionTreeClassifier\n",
    "# param_grid = {'max_depth': [2, 4, 6, 8]}\n",
    "# grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "[Implement the final model(s) you've selected based on the above steps.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "[Clearly specify which metrics you'll use to evaluate the model performance, and why you've chosen these metrics.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "[Compare the performance of your model(s) against the baseline model. Discuss any improvements or setbacks and the reasons behind them.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative Analysis code (if applicable)\n",
    "# Example: comparing accuracy of the baseline model and the new model\n",
    "# print(f\"Baseline Model Accuracy: {baseline_accuracy}, New Model Accuracy: {new_model_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
