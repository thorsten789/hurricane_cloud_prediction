{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4865d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Imports\n",
    "# =========================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from huggingface_hub import login, list_repo_files, hf_hub_download, upload_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c5636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Konfiguration\n",
    "# =========================================\n",
    "REPO_ID = \"mttfst/Paulette_Cloud_Tracks\"\n",
    "token = \"\"\n",
    "\n",
    "PAD_VALUE = -99.0\n",
    "TIMESTEP_SECONDS = 30.0\n",
    "CUTOFF_STEPS = 5\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 2 #40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d017f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Login HuggingFace\n",
    "# =========================================\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31bb6bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_ID: simple_rnn_2026-02-09_18-18-15_3d1caad569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HF] uploaded: runs/simple_rnn_2026-02-09_18-18-15_3d1caad569/setup.json\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Logging\n",
    "# =========================================\n",
    "import os, json, hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "# Du kannst hier ein separates Repo für Logs/Configs nutzen (empfohlen),\n",
    "# oder du lässt es auf dem Dataset-Repo laufen.\n",
    "CONFIG_REPO_ID = REPO_ID  # z.B. \"thorsten789/hurricane_cloud_runs\"\n",
    "\n",
    "def make_run_id(prefix: str, config: dict) -> str:\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    cfg_str = json.dumps(config, sort_keys=True)\n",
    "    h = hashlib.sha1(cfg_str.encode(\"utf-8\")).hexdigest()[:10]\n",
    "    return f\"{prefix}_{ts}_{h}\"\n",
    "\n",
    "def save_json_local(path: str, data: dict) -> str:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    return path\n",
    "\n",
    "def upload_json_hf(local_path: str, run_id: str, name: str, base_dir: str = \"runs\"):\n",
    "    \"\"\"Lädt JSON als {base_dir}/{run_id}/{name}.json in CONFIG_REPO_ID hoch.\"\"\"\n",
    "    try:\n",
    "        path_in_repo = f\"{base_dir}/{run_id}/{name}.json\"\n",
    "        upload_file(\n",
    "            path_or_fileobj=local_path,\n",
    "            path_in_repo=path_in_repo,\n",
    "            repo_id=CONFIG_REPO_ID,\n",
    "            repo_type=\"dataset\",\n",
    "            commit_message=f\"Add {name}.json for {run_id}\",\n",
    "        )\n",
    "        print(f\"[HF] uploaded: {path_in_repo}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[HF] upload skipped/failed ({name}): {e}\")\n",
    "\n",
    "# --- zentrale Run-Config (die ID basiert auf config -> sinnvoller Run-Name)\n",
    "RUN_CONFIG = {\n",
    "    \"model\": \"LSTM\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"units1\": 64,\n",
    "    \"units2\": 32,\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"cutoff_steps\": CUTOFF_STEPS,\n",
    "    \"pad_value\": PAD_VALUE,\n",
    "}\n",
    "\n",
    "RUN_ID = make_run_id(\"simple_rnn\", RUN_CONFIG)\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "\n",
    "# Setup sofort speichern (damit du schon am Anfang einen Run hast)\n",
    "setup = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"config\": RUN_CONFIG,\n",
    "    \"data\": {\"repo_id\": REPO_ID},\n",
    "    \"meta\": {\"notebook\": \"3_Model/lstm_baseline_JS.ipynb\"},\n",
    "}\n",
    "\n",
    "# +++ Logging: Save Setup +++\n",
    "local_setup = save_json_local(f\"runs_local/{RUN_ID}/setup.json\", setup)\n",
    "upload_json_hf(local_setup, RUN_ID, \"setup\")\n",
    "\n",
    "class AutoSaveTrain(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Speichert train.json am Ende von model.fit (History + best_val_loss).\"\"\"\n",
    "    def __init__(self, run_id: str):\n",
    "        super().__init__()\n",
    "        self.run_id = run_id\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        hist = getattr(self.model, \"history\", None)\n",
    "        history_dict = hist.history if hist is not None else {}\n",
    "\n",
    "        train_data = {\n",
    "            \"run_id\": self.run_id,\n",
    "            \"history\": history_dict,\n",
    "            \"summary\": {\n",
    "                \"best_val_loss\": float(min(history_dict[\"val_loss\"])) if \"val_loss\" in history_dict else None,\n",
    "                \"final_train_loss\": float(history_dict[\"loss\"][-1]) if \"loss\" in history_dict and len(history_dict[\"loss\"]) else None,\n",
    "            },\n",
    "        }\n",
    "        local_train = save_json_local(f\"runs_local/{self.run_id}/train.json\", train_data)\n",
    "        upload_json_hf(local_train, self.run_id, \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee255591",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILE_PREFIXES = [\"qr_\", \"qc_\", \"qi_\", \"qs_\", \"qg_\", \"qv_\", \"roh_\", \"w_\"]\n",
    "\n",
    "SCALAR_FEATURES = [\n",
    "    #\"cape_ml_L00\", \"cin_ml_L00\",\n",
    "    \"lwp_L00\",\n",
    "    \"iwp_L00\", \"rain_gsp_rate_L00\",\n",
    "    \"tqc_L00\", \"tqi_L00\", \"area_m2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360aa8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️  Downloading from HF: mttfst/Paulette_Cloud_Tracks/track_len/track_len_exp_1.1.csv\n",
      "✅ Loading track_len from HF-cached file: /root/.cache/huggingface/hub/datasets--mttfst--Paulette_Cloud_Tracks/snapshots/13153ed2f2d981c249bc7287e95ac63dbb171914/track_len/track_len_exp_1.1.csv\n",
      "tracks_120 shape: (9227, 2)\n",
      "Total CSV tracks with at least 120 timesteps: 8119\n",
      "Train: 5683, Val: 1218, Test: 1218\n"
     ]
    }
   ],
   "source": [
    "tracks_120 = None\n",
    "\n",
    "# Wenn lokal nicht da: von Hugging Face Dataset-Repo herunterladen\n",
    "if tracks_120 is None:\n",
    "    from huggingface_hub import hf_hub_download\n",
    "\n",
    "    filename_in_repo = \"track_len/track_len_exp_1.1.csv\"\n",
    "\n",
    "    print(f\"⬇️  Downloading from HF: {REPO_ID}/{filename_in_repo}\")\n",
    "    local_file = hf_hub_download(\n",
    "        repo_id=REPO_ID,\n",
    "        repo_type=\"dataset\",\n",
    "        filename=filename_in_repo,\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Loading track_len from HF-cached file: {local_file}\")\n",
    "    tracks_120 = pd.read_csv(local_file)\n",
    "\n",
    "print(\"tracks_120 shape:\", tracks_120.shape)\n",
    "tracks_120.head()\n",
    "\n",
    "# =========================================\n",
    "# Dataset-Split\n",
    "# =========================================\n",
    "files = list_repo_files(REPO_ID, repo_type=\"dataset\")\n",
    "csv_files = [f for f in files if f.startswith(\"exp_1.1/\") and f.endswith(\".csv\")]\n",
    "\n",
    "# tracks_120 = pd.read_csv(\"/content/hurricane_cloud_prediction/5_Data_Sample/track_len.csv\")\n",
    "\n",
    "tracks_120 = tracks_120[tracks_120.track_len <= 120]\n",
    "print(\"Total CSV tracks with at least 120 timesteps:\", len(tracks_120))\n",
    "\n",
    "tracks_120 = tracks_120.filename.to_list()\n",
    "\n",
    "csv_files = [\n",
    "    f for f in csv_files\n",
    "    if f.split(\"/\")[1] in tracks_120\n",
    "]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "n = len(csv_files)\n",
    "train_files = csv_files[: int(0.7 * n)]\n",
    "val_files   = csv_files[int(0.7 * n): int(0.85 * n)]\n",
    "test_files  = csv_files[int(0.85 * n):]\n",
    "\n",
    "print(f\"Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d886bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_remaining_lifetime(df, timestep_minutes=5):\n",
    "    \"\"\"\n",
    "    Berechnet verbleibende Lebensdauer pro Zeitschritt\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    return [(n - i - 1) * timestep_minutes for i in range(n)]\n",
    "\n",
    "#def compute_future_rain(df, timestep_minutes=5):\n",
    "#    rain = df[\"rain_gsp_rate_L00\"].values\n",
    "#    dt = timestep_minutes * 60  # Sekunden\n",
    "\n",
    "#    future_rain = []\n",
    "#    for i in range(len(rain)):\n",
    "#        future_rain.append(rain[i:].sum() * dt)\n",
    "\n",
    "#    return future_rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db53973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_profile(df, prefix, n_levels=50):\n",
    "    \"\"\"\n",
    "    Extrahiert ein vertikales Profil mit exakt n_levels.\n",
    "    Fehlende Level werden mit 0 aufgefüllt.\n",
    "    \"\"\"\n",
    "    data = np.zeros((len(df), n_levels), dtype=\"float32\")\n",
    "    for i in range(n_levels):\n",
    "        col = f\"{prefix}L{i:02d}\"\n",
    "        if col in df.columns:\n",
    "            data[:, i] = df[col].values\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bf34a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cloud(df):\n",
    "    df = df.sort_values(\"time\")\n",
    "\n",
    "    # Target\n",
    "    y_lifetime = compute_remaining_lifetime(df)\n",
    "\n",
    "    # Profile (T, Z, F)\n",
    "    profile_features = []\n",
    "    for prefix in PROFILE_PREFIXES:\n",
    "        prof = extract_profile(df, prefix, n_levels=50)\n",
    "        profile_features.append(prof)\n",
    "    \n",
    "    profiles = np.stack(profile_features, axis=-1)\n",
    "    \n",
    "    # Scalars\n",
    "    scalars = df[SCALAR_FEATURES].values.astype(\"float32\")\n",
    "    \n",
    "    return {\n",
    "        \"profiles\": profiles,\n",
    "        \"scalars\": scalars,\n",
    "        \"y\": np.array(y_lifetime, dtype=\"float32\")[:, None]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3abe5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(files):\n",
    "    samples = []\n",
    "    for f in files:\n",
    "        local_file = hf_hub_download(\n",
    "            repo_id=REPO_ID,\n",
    "            repo_type=\"dataset\",\n",
    "            filename=f,\n",
    "        )\n",
    "        df = pd.read_csv(local_file)\n",
    "        \n",
    "        if len(df) <= CUTOFF_STEPS:\n",
    "            continue\n",
    "        \n",
    "        sample = preprocess_cloud(df)\n",
    "        samples.append(sample)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc585705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading data...\n",
      "Loaded: 4036 train, 869 val, 864 test\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Loading data...\")\n",
    "train_samples = load_and_preprocess(train_files)\n",
    "val_samples   = load_and_preprocess(val_files)\n",
    "test_samples  = load_and_preprocess(test_files)\n",
    "\n",
    "print(f\"Loaded: {len(train_samples)} train, {len(val_samples)} val, {len(test_samples)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d888092a",
   "metadata": {},
   "source": [
    "## Normalisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a543edad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fitting Scalers...\n",
      "3 constant profile features found\n",
      "✅ Scalers fitted\n",
      "\n",
      " Normalizing targets...\n",
      "Target Stats - Mean: 131.75 min, Std: 122.69 min\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Fitting Scalers...\")\n",
    "\n",
    "# Scaler für Scalars\n",
    "scalar_scaler = StandardScaler()\n",
    "all_train_scalars = np.concatenate([s[\"scalars\"] for s in train_samples], axis=0)\n",
    "\n",
    "# Check for NaN\n",
    "nan_count = np.isnan(all_train_scalars).sum(axis=0)\n",
    "if nan_count.sum() > 0:\n",
    "    print(f\"Warning: {nan_count.sum()} NaN values in scalars\")\n",
    "    # Ersetze NaN mit 0\n",
    "    all_train_scalars = np.nan_to_num(all_train_scalars, nan=0.0)\n",
    "\n",
    "scalar_scaler.fit(all_train_scalars)\n",
    "\n",
    "# Scaler für Profiles\n",
    "profile_scaler = StandardScaler()\n",
    "all_train_profiles = np.concatenate(\n",
    "    [s[\"profiles\"].reshape(-1, s[\"profiles\"].shape[-1]) for s in train_samples],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "# Check for constant features\n",
    "variances = np.var(all_train_profiles, axis=0)\n",
    "constant_mask = variances < 1e-8\n",
    "if constant_mask.any():\n",
    "    print(f\"{constant_mask.sum()} constant profile features found\")\n",
    "\n",
    "profile_scaler.fit(all_train_profiles)\n",
    "\n",
    "# Fix constant features (set scale to 1.0 to avoid NaN)\n",
    "profile_scaler.scale_[constant_mask] = 1.0\n",
    "\n",
    "print(\"✅ Scalers fitted\")\n",
    "\n",
    "# =========================================\n",
    "# Target (Y) Normalisierung\n",
    "# =========================================\n",
    "print(\"\\n Normalizing targets...\")\n",
    "\n",
    "all_train_y = np.concatenate([s[\"y\"] for s in train_samples], axis=0)\n",
    "y_mean = np.mean(all_train_y)\n",
    "y_std = np.std(all_train_y)\n",
    "\n",
    "print(f\"Target Stats - Mean: {y_mean:.2f} min, Std: {y_std:.2f} min\")\n",
    "\n",
    "# =========================================\n",
    "# Apply Normalization to Samples\n",
    "# =========================================\n",
    "def normalize_sample(sample, scalar_scaler, profile_scaler, y_mean, y_std):\n",
    "    sample_norm = sample.copy()\n",
    "    \n",
    "    # Normalize scalars\n",
    "    sample_norm[\"scalars\"] = scalar_scaler.transform(sample[\"scalars\"])\n",
    "    \n",
    "    # Normalize profiles (reshape, transform, reshape back)\n",
    "    T, Z, F = sample[\"profiles\"].shape\n",
    "    prof_flat = sample[\"profiles\"].reshape(-1, F)\n",
    "    prof_norm = profile_scaler.transform(prof_flat)\n",
    "    sample_norm[\"profiles\"] = prof_norm.reshape(T, Z, F)\n",
    "    \n",
    "    # Normalize targets\n",
    "    sample_norm[\"y\"] = (sample[\"y\"] - y_mean) / y_std\n",
    "    \n",
    "    return sample_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba8710f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying normalization...\n",
      "Normalization applied\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying normalization...\")\n",
    "train_samples_norm = [normalize_sample(s, scalar_scaler, profile_scaler, y_mean, y_std) \n",
    "                      for s in train_samples]\n",
    "val_samples_norm = [normalize_sample(s, scalar_scaler, profile_scaler, y_mean, y_std) \n",
    "                    for s in val_samples]\n",
    "test_samples_norm = [normalize_sample(s, scalar_scaler, profile_scaler, y_mean, y_std) \n",
    "                     for s in test_samples]\n",
    "\n",
    "print(\"Normalization applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002596c",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12330775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(samples):\n",
    "    max_len = max(s[\"profiles\"].shape[0] for s in samples)\n",
    "    \n",
    "    def pad(arr, pad_width):\n",
    "        return np.pad(arr, pad_width, mode=\"constant\", constant_values=PAD_VALUE)\n",
    "    \n",
    "    X_profiles, X_scalars, Y = [], [], []\n",
    "    \n",
    "    for s in samples:\n",
    "        T = s[\"profiles\"].shape[0]\n",
    "        \n",
    "        X_profiles.append(pad(s[\"profiles\"], ((0, max_len-T), (0,0), (0,0))))\n",
    "        X_scalars.append(pad(s[\"scalars\"], ((0, max_len-T), (0,0))))\n",
    "        Y.append(pad(s[\"y\"], ((0, max_len-T), (0,0))))\n",
    "    \n",
    "    return (np.array(X_profiles, dtype=np.float32), \n",
    "            np.array(X_scalars, dtype=np.float32), \n",
    "            np.array(Y, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08e5e779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Padding sequences...\n",
      "Padded shapes:\n",
      "   Profiles: (4036, 120, 50, 8)\n",
      "   Scalars: (4036, 120, 6)\n",
      "   Targets: (4036, 120, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Padding sequences...\")\n",
    "Xprof_train, Xsca_train, Y_train = pad_sequences(train_samples_norm)\n",
    "Xprof_val, Xsca_val, Y_val = pad_sequences(val_samples_norm)\n",
    "Xprof_test, Xsca_test, Y_test = pad_sequences(test_samples_norm)\n",
    "\n",
    "print(f\"Padded shapes:\")\n",
    "print(f\"   Profiles: {Xprof_train.shape}\")\n",
    "print(f\"   Scalars: {Xsca_train.shape}\")\n",
    "print(f\"   Targets: {Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a661ab3",
   "metadata": {},
   "source": [
    "# Custom Loss + Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef9556a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse_loss(pad_value=PAD_VALUE):\n",
    "    def loss(y_true, y_pred):\n",
    "        mask = tf.not_equal(y_true, pad_value)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        \n",
    "        squared_error = tf.square(y_true - y_pred)\n",
    "        masked_error = squared_error * mask\n",
    "        \n",
    "        sum_error = tf.reduce_sum(masked_error)\n",
    "        count = tf.reduce_sum(mask)\n",
    "        count = tf.maximum(count, 1.0)\n",
    "        \n",
    "        return sum_error / count\n",
    "    return loss\n",
    "\n",
    "def masked_mae_metric(pad_value=PAD_VALUE):\n",
    "    def metric(y_true, y_pred):\n",
    "        mask = tf.not_equal(y_true, pad_value)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        \n",
    "        abs_error = tf.abs(y_true - y_pred)\n",
    "        masked_error = abs_error * mask\n",
    "        \n",
    "        sum_error = tf.reduce_sum(masked_error)\n",
    "        count = tf.reduce_sum(mask)\n",
    "        count = tf.maximum(count, 1.0)\n",
    "        \n",
    "        return sum_error / count\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8573d8b6",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3ea44e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Building model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Hybrid_CNN_LSTM\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Hybrid_CNN_LSTM\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ profile_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span> │ profile_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ scalar_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1606</span>)             │            │ scalar_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1606</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1606</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ any (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Any</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">888,320</span> │ masking[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_norm_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ batch_norm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_norm_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_hidden        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ batch_norm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lifetime_output     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dense_hidden[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ profile_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m8\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m,  │      \u001b[38;5;34m1,600\u001b[0m │ profile_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m,  │      \u001b[38;5;34m6,176\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m1600\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ scalar_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m1606\u001b[0m)             │            │ scalar_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │ \u001b[38;5;34m1606\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ masking (\u001b[38;5;33mMasking\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m1606\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ any (\u001b[38;5;33mAny\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m888,320\u001b[0m │ masking[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_norm_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m49,408\u001b[0m │ batch_norm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_norm_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_hidden        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m2,080\u001b[0m │ batch_norm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lifetime_output     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │         \u001b[38;5;34m33\u001b[0m │ dense_hidden[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">948,385</span> (3.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m948,385\u001b[0m (3.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">948,001</span> (3.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m948,001\u001b[0m (3.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_hybrid_model(z_levels=50, n_profile_features=8, n_scalars=6):\n",
    "    # Profile Input\n",
    "    profile_input = layers.Input(shape=(None, z_levels, n_profile_features), \n",
    "                                 name='profile_input')\n",
    "    \n",
    "    # CNN für vertikale Features\n",
    "    x = layers.TimeDistributed(\n",
    "        layers.Conv1D(64, kernel_size=3, activation=\"relu\", padding=\"same\"),\n",
    "        name='conv1d_1'\n",
    "    )(profile_input)\n",
    "    \n",
    "    x = layers.TimeDistributed(\n",
    "        layers.Conv1D(32, kernel_size=3, activation=\"relu\", padding=\"same\"),\n",
    "        name='conv1d_2'\n",
    "    )(x)\n",
    "    \n",
    "    x = layers.TimeDistributed(layers.Flatten(), name='flatten')(x)\n",
    "    \n",
    "    # Scalar Input\n",
    "    scalar_input = layers.Input(shape=(None, n_scalars), name='scalar_input')\n",
    "    \n",
    "    # Concatenate\n",
    "    x = layers.Concatenate(name='concatenate')([x, scalar_input])\n",
    "    \n",
    "    # Masking\n",
    "    x = layers.Masking(mask_value=PAD_VALUE, name='masking')(x)\n",
    "    \n",
    "    # LSTM Layers\n",
    "    x = layers.LSTM(128, return_sequences=True, \n",
    "                   dropout=0.3, recurrent_dropout=0.2,\n",
    "                   name='lstm_1')(x)\n",
    "    x = layers.BatchNormalization(name='batch_norm_1')(x)\n",
    "    \n",
    "    x = layers.LSTM(64, return_sequences=True,\n",
    "                   dropout=0.3, recurrent_dropout=0.2,\n",
    "                   name='lstm_2')(x)\n",
    "    x = layers.BatchNormalization(name='batch_norm_2')(x)\n",
    "    \n",
    "    # Dense Layer\n",
    "    x = layers.TimeDistributed(\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        name='dense_hidden'\n",
    "    )(x)\n",
    "    \n",
    "    # Output\n",
    "    lifetime_out = layers.TimeDistributed(\n",
    "        layers.Dense(1), \n",
    "        name=\"lifetime_output\"\n",
    "    )(x)\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[profile_input, scalar_input],\n",
    "        outputs=lifetime_out,\n",
    "        name='Hybrid_CNN_LSTM'\n",
    "    )\n",
    "    \n",
    "    # Compile with masked loss\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=1e-3,\n",
    "        clipnorm=1.0  # Gradient clipping\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=masked_mse_loss(PAD_VALUE),\n",
    "        metrics=[masked_mae_metric(PAD_VALUE)]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"\\n Building model...\")\n",
    "model = build_hybrid_model(\n",
    "    z_levels=50,\n",
    "    n_profile_features=len(PROFILE_PREFIXES),\n",
    "    n_scalars=len(SCALAR_FEATURES)\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b73a6e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd050676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train model...\n",
      "Epoch 1/2\n",
      "\u001b[1m505/505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1052s\u001b[0m 2s/step - loss: 0.7289 - metric: 0.6324 - val_loss: 0.8261 - val_metric: 0.6670 - learning_rate: 0.0010\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "[HF] uploaded: runs/simple_rnn_2026-02-09_18-18-15_3d1caad569/train.json\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.TerminateOnNaN(),\n",
    "    AutoSaveTrain(RUN_ID)\n",
    "]\n",
    "\n",
    "print(\"\\n Train model...\")\n",
    "\n",
    "history = model.fit(\n",
    "    [Xprof_train, Xsca_train], Y_train,\n",
    "    validation_data=([Xprof_val, Xsca_val], Y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666b04b",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c35b85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 257ms/step - loss: 0.8058 - metric: 0.6572\n",
      "\n",
      "Test Loss (normalized): 0.8399\n",
      "Test MAE (normalized): 0.6664\n",
      "\n",
      " Real-world Metrics:\n",
      "MAE:  86.97 minutes\n",
      "RMSE: 120.28 minutes\n",
      "[HF] uploaded: runs/simple_rnn_2026-02-09_18-18-15_3d1caad569/eval.json\n",
      "\n",
      " Run simple_rnn_2026-02-09_18-18-15_3d1caad569 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_results = model.evaluate(\n",
    "    [Xprof_test, Xsca_test], Y_test, \n",
    "    verbose=1,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Loss (normalized): {test_results[0]:.4f}\")\n",
    "print(f\"Test MAE (normalized): {test_results[1]:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_norm = model.predict(\n",
    "    [Xprof_test, Xsca_test], \n",
    "    verbose=0,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Denormalize\n",
    "def denormalize_predictions(y_norm, y_mean, y_std, pad_value=PAD_VALUE):\n",
    "    y = y_norm.copy()\n",
    "    mask = y != pad_value\n",
    "    y[mask] = y[mask] * y_std + y_mean\n",
    "    return y\n",
    "\n",
    "y_pred = denormalize_predictions(y_pred_norm, y_mean, y_std)\n",
    "y_test_denorm = denormalize_predictions(Y_test, y_mean, y_std)\n",
    "\n",
    "# Real-world metrics\n",
    "mask = y_test_denorm != PAD_VALUE\n",
    "mae_minutes = np.mean(np.abs(y_test_denorm[mask] - y_pred[mask]))\n",
    "rmse_minutes = np.sqrt(np.mean((y_test_denorm[mask] - y_pred[mask])**2))\n",
    "\n",
    "print(f\"\\n Real-world Metrics:\")\n",
    "print(f\"MAE:  {mae_minutes:.2f} minutes\")\n",
    "print(f\"RMSE: {rmse_minutes:.2f} minutes\")\n",
    "\n",
    "# =========================================\n",
    "# Save Evaluation\n",
    "# =========================================\n",
    "eval_data = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"metrics\": {\n",
    "        \"test_loss_norm\": float(test_results[0]),\n",
    "        \"test_mae_norm\": float(test_results[1]),\n",
    "        \"mae_minutes\": float(mae_minutes),\n",
    "        \"rmse_minutes\": float(rmse_minutes),\n",
    "    }\n",
    "}\n",
    "\n",
    "local_eval = save_json_local(f\"runs_local/{RUN_ID}/eval.json\", eval_data)\n",
    "upload_json_hf(local_eval, RUN_ID, \"eval\")\n",
    "\n",
    "print(f\"\\n Run {RUN_ID} completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
